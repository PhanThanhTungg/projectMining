{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "df206667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import heapq\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45952b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistogramBuilder:\n",
    "    def __init__(self, max_bins=255):\n",
    "        self.max_bins = max_bins\n",
    "        self.bin_mappers = {}  # Lưu mapping từ feature value sang bin\n",
    "        \n",
    "    def build_feature_histograms(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            feature_values = X[:, feature_idx]\n",
    "            unique_values = np.unique(feature_values)\n",
    "            \n",
    "            if len(unique_values) <= self.max_bins:\n",
    "                # Nếu unique values ít, dùng exact values\n",
    "                bin_edges = np.concatenate([unique_values, [unique_values[-1] + 1e-10]])\n",
    "            else:\n",
    "                # Tạo equal-density histogram\n",
    "                bin_edges = np.histogram_bin_edges(feature_values, bins=self.max_bins)\n",
    "            \n",
    "            self.bin_mappers[feature_idx] = bin_edges\n",
    "    \n",
    "    def map_to_bins(self, X):\n",
    "        n_samples, n_features = X.shape\n",
    "        binned_X = np.zeros((n_samples, n_features), dtype=np.uint8)\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            bin_edges = self.bin_mappers[feature_idx]\n",
    "            binned_X[:, feature_idx] = np.digitize(X[:, feature_idx], bin_edges) - 1\n",
    "            binned_X[:, feature_idx] = np.clip(binned_X[:, feature_idx], 0, len(bin_edges) - 2)\n",
    "        \n",
    "        return binned_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28db9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientHistogram:\n",
    "    def __init__(self, n_bins):\n",
    "        self.n_bins = n_bins\n",
    "        self.gradient_sum = np.zeros(n_bins)\n",
    "        self.hessian_sum = np.zeros(n_bins)\n",
    "        self.sample_count = np.zeros(n_bins)\n",
    "    \n",
    "    def add_sample(self, bin_idx, gradient, hessian):\n",
    "        self.gradient_sum[bin_idx] += gradient\n",
    "        self.hessian_sum[bin_idx] += hessian\n",
    "        self.sample_count[bin_idx] += 1\n",
    "    \n",
    "    def calculate_gain(self, reg_lambda=0.1):\n",
    "        gains = []\n",
    "        \n",
    "        # Cumulative sums for left side\n",
    "        cumsum_grad = np.cumsum(self.gradient_sum)\n",
    "        cumsum_hess = np.cumsum(self.hessian_sum)\n",
    "        cumsum_count = np.cumsum(self.sample_count)\n",
    "        \n",
    "        total_grad = cumsum_grad[-1]\n",
    "        total_hess = cumsum_hess[-1]\n",
    "        total_count = cumsum_count[-1]\n",
    "        \n",
    "        for split_idx in range(self.n_bins - 1):\n",
    "            left_grad = cumsum_grad[split_idx]\n",
    "            left_hess = cumsum_hess[split_idx]\n",
    "            left_count = cumsum_count[split_idx]\n",
    "            \n",
    "            right_grad = total_grad - left_grad\n",
    "            right_hess = total_hess - left_hess\n",
    "            right_count = total_count - left_count\n",
    "            \n",
    "            if left_count < 1 or right_count < 1:\n",
    "                gains.append(-np.inf)\n",
    "                continue\n",
    "            \n",
    "            # LightGBM gain formula\n",
    "            left_gain = (left_grad ** 2) / (left_hess + reg_lambda)\n",
    "            right_gain = (right_grad ** 2) / (right_hess + reg_lambda)\n",
    "            parent_gain = (total_grad ** 2) / (total_hess + reg_lambda)\n",
    "            \n",
    "            gain = 0.5 * (left_gain + right_gain - parent_gain)\n",
    "            gains.append(gain)\n",
    "        \n",
    "        return np.array(gains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMTreeNode:\n",
    "    \n",
    "    def __init__(self, depth=0, samples_idx=None):\n",
    "        # Split information\n",
    "        self.feature_idx = None\n",
    "        self.bin_threshold = None  # Threshold ở dạng bin index\n",
    "        self.raw_threshold = None  # Threshold ở dạng raw value\n",
    "        \n",
    "        # Tree structure\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.parent = None\n",
    "        \n",
    "        # Node information\n",
    "        self.is_leaf = True\n",
    "        self.depth = depth\n",
    "        self.samples_idx = samples_idx if samples_idx is not None else []\n",
    "        self.value = 0.0  # Leaf value\n",
    "        \n",
    "        # Statistics for gain calculation\n",
    "        self.gradient_sum = 0.0\n",
    "        self.hessian_sum = 0.0\n",
    "        self.sample_count = 0\n",
    "        \n",
    "        # Priority for leaf-wise growth\n",
    "        self.split_gain = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8168a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMTree:\n",
    "    \n",
    "    def __init__(self, max_depth=6, min_child_samples=20, reg_lambda=0.1, \n",
    "                 histogram_builder=None, max_leaves=31):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.histogram_builder = histogram_builder\n",
    "        self.max_leaves = max_leaves\n",
    "        \n",
    "        self.root = None\n",
    "        self.leaf_nodes = []  # Danh sách các leaf có thể split\n",
    "        \n",
    "    def fit(self, X_binned, gradients, hessians):\n",
    "        \"\"\"Train tree với leaf-wise approach\"\"\"\n",
    "        n_samples = len(gradients)\n",
    "        \n",
    "        # Initialize root\n",
    "        self.root = LightGBMTreeNode(depth=0, samples_idx=np.arange(n_samples))\n",
    "        self._update_node_stats(self.root, gradients, hessians)\n",
    "        \n",
    "        # Priority queue cho leaf-wise growth\n",
    "        self.leaf_nodes = [self.root]\n",
    "        \n",
    "        num_leaves = 1\n",
    "        while num_leaves < self.max_leaves and self.leaf_nodes:\n",
    "            # Tìm leaf có highest gain để split\n",
    "            best_leaf, best_split = self._find_best_leaf_to_split(X_binned, gradients, hessians)\n",
    "            \n",
    "            if best_leaf is None or best_split['gain'] <= 0:\n",
    "                break\n",
    "            \n",
    "            # Split best leaf\n",
    "            left_child, right_child = self._split_leaf(best_leaf, best_split, \n",
    "                                                     X_binned, gradients, hessians)\n",
    "            \n",
    "            # Remove split leaf và add new leaves\n",
    "            self.leaf_nodes.remove(best_leaf)\n",
    "            if left_child.depth < self.max_depth:\n",
    "                self.leaf_nodes.append(left_child)\n",
    "            if right_child.depth < self.max_depth:\n",
    "                self.leaf_nodes.append(right_child)\n",
    "            \n",
    "            num_leaves += 1\n",
    "        \n",
    "        # Calculate leaf values\n",
    "        self._calculate_leaf_values()\n",
    "    \n",
    "    def _update_node_stats(self, node, gradients, hessians):\n",
    "        \"\"\"Update gradient/hessian statistics cho node\"\"\"\n",
    "        if len(node.samples_idx) > 0:\n",
    "            node.gradient_sum = np.sum(gradients[node.samples_idx])\n",
    "            node.hessian_sum = np.sum(hessians[node.samples_idx])\n",
    "            node.sample_count = len(node.samples_idx)\n",
    "    \n",
    "    def _find_best_leaf_to_split(self, X_binned, gradients, hessians):\n",
    "        \"\"\"Tìm leaf tốt nhất để split\"\"\"\n",
    "        best_leaf = None\n",
    "        best_split = {'gain': -np.inf}\n",
    "        \n",
    "        for leaf in self.leaf_nodes:\n",
    "            if (leaf.sample_count < self.min_child_samples * 2 or \n",
    "                leaf.depth >= self.max_depth):\n",
    "                continue\n",
    "            \n",
    "            split_info = self._find_best_split_for_leaf(leaf, X_binned, gradients, hessians)\n",
    "            if split_info['gain'] > best_split['gain']:\n",
    "                best_split = split_info\n",
    "                best_leaf = leaf\n",
    "        \n",
    "        return best_leaf, best_split\n",
    "    \n",
    "    def _find_best_split_for_leaf(self, leaf, X_binned, gradients, hessians):\n",
    "        \"\"\"Tìm best split cho một leaf sử dụng histogram\"\"\"\n",
    "        best_split = {'gain': -np.inf}\n",
    "        \n",
    "        n_features = X_binned.shape[1]\n",
    "        samples_idx = leaf.samples_idx\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            # Build histogram cho feature này\n",
    "            feature_bins = X_binned[samples_idx, feature_idx]\n",
    "            max_bin = int(np.max(feature_bins)) + 1\n",
    "            \n",
    "            hist = GradientHistogram(max_bin)\n",
    "            \n",
    "            # Populate histogram\n",
    "            for i, sample_idx in enumerate(samples_idx):\n",
    "                bin_idx = feature_bins[i]\n",
    "                hist.add_sample(bin_idx, gradients[sample_idx], hessians[sample_idx])\n",
    "            \n",
    "            # Calculate gains for all possible splits\n",
    "            gains = hist.calculate_gain(self.reg_lambda)\n",
    "            \n",
    "            if len(gains) > 0:\n",
    "                best_bin_idx = np.argmax(gains)\n",
    "                max_gain = gains[best_bin_idx]\n",
    "                \n",
    "                if max_gain > best_split['gain']:\n",
    "                    # Convert bin threshold to raw value\n",
    "                    bin_edges = self.histogram_builder.bin_mappers[feature_idx]\n",
    "                    raw_threshold = bin_edges[best_bin_idx + 1]\n",
    "                    \n",
    "                    best_split = {\n",
    "                        'gain': max_gain,\n",
    "                        'feature_idx': feature_idx,\n",
    "                        'bin_threshold': best_bin_idx,\n",
    "                        'raw_threshold': raw_threshold\n",
    "                    }\n",
    "        \n",
    "        return best_split\n",
    "    \n",
    "    def _split_leaf(self, leaf, split_info, X_binned, gradients, hessians):\n",
    "        \"\"\"Split một leaf thành 2 children\"\"\"\n",
    "        feature_idx = split_info['feature_idx']\n",
    "        bin_threshold = split_info['bin_threshold']\n",
    "        \n",
    "        # Split samples\n",
    "        samples_idx = leaf.samples_idx\n",
    "        feature_bins = X_binned[samples_idx, feature_idx]\n",
    "        \n",
    "        left_mask = feature_bins <= bin_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_samples = samples_idx[left_mask]\n",
    "        right_samples = samples_idx[right_mask]\n",
    "        \n",
    "        # Create children\n",
    "        left_child = LightGBMTreeNode(depth=leaf.depth + 1, samples_idx=left_samples)\n",
    "        right_child = LightGBMTreeNode(depth=leaf.depth + 1, samples_idx=right_samples)\n",
    "        \n",
    "        # Update statistics\n",
    "        self._update_node_stats(left_child, gradients, hessians)\n",
    "        self._update_node_stats(right_child, gradients, hessians)\n",
    "        \n",
    "        # Update parent\n",
    "        leaf.is_leaf = False\n",
    "        leaf.feature_idx = feature_idx\n",
    "        leaf.bin_threshold = bin_threshold\n",
    "        leaf.raw_threshold = split_info['raw_threshold']\n",
    "        leaf.left = left_child\n",
    "        leaf.right = right_child\n",
    "        \n",
    "        left_child.parent = leaf\n",
    "        right_child.parent = leaf\n",
    "        \n",
    "        return left_child, right_child\n",
    "    \n",
    "    def _calculate_leaf_values(self):\n",
    "        \"\"\"Tính leaf values sử dụng gradient/hessian\"\"\"\n",
    "        self._calculate_leaf_values_recursive(self.root)\n",
    "    \n",
    "    def _calculate_leaf_values_recursive(self, node):\n",
    "        \"\"\"Recursively calculate leaf values\"\"\"\n",
    "        if node.is_leaf:\n",
    "            if node.hessian_sum > 0:\n",
    "                node.value = -node.gradient_sum / (node.hessian_sum + self.reg_lambda)\n",
    "            else:\n",
    "                node.value = 0.0\n",
    "        else:\n",
    "            self._calculate_leaf_values_recursive(node.left)\n",
    "            self._calculate_leaf_values_recursive(node.right)\n",
    "    \n",
    "    def predict(self, X_binned):\n",
    "        \"\"\"Predict với binned features\"\"\"\n",
    "        return np.array([self._predict_sample(x) for x in X_binned])\n",
    "    \n",
    "    def _predict_sample(self, x_binned):\n",
    "        \"\"\"Predict single sample\"\"\"\n",
    "        node = self.root\n",
    "        while not node.is_leaf:\n",
    "            if x_binned[node.feature_idx] <= node.bin_threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        return node.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a024f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGBMClassifier:\n",
    "    def __init__(self, \n",
    "                 n_estimators=100,\n",
    "                 learning_rate=0.1,\n",
    "                 max_depth=6,\n",
    "                 num_leaves=31,\n",
    "                 min_child_samples=20,\n",
    "                 subsample=1.0,\n",
    "                 colsample_bytree=1.0,\n",
    "                 reg_lambda=0.1,\n",
    "                 max_bins=255,\n",
    "                 random_state=None):\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.num_leaves = num_leaves\n",
    "        self.min_child_samples = min_child_samples\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.max_bins = max_bins\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        # Model components\n",
    "        self.histogram_builder = HistogramBuilder(max_bins=max_bins)\n",
    "        self.trees = []\n",
    "        self.feature_indices = []\n",
    "        \n",
    "        # Multiclass setup\n",
    "        self.n_classes = None\n",
    "        self.classes_ = None\n",
    "        self.initial_predictions = None\n",
    "        \n",
    "        if random_state:\n",
    "            np.random.seed(random_state)\n",
    "    \n",
    "    def _softmax(self, x):\n",
    "        \"\"\"Numerically stable softmax\"\"\"\n",
    "        x_shifted = x - np.max(x, axis=1, keepdims=True)\n",
    "        exp_x = np.exp(np.clip(x_shifted, -500, 500))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    \n",
    "    def _one_hot_encode(self, y):\n",
    "        \"\"\"Convert labels to one-hot encoding\"\"\"\n",
    "        n_samples = len(y)\n",
    "        one_hot = np.zeros((n_samples, self.n_classes))\n",
    "        one_hot[np.arange(n_samples), y] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def _compute_gradients_hessians(self, y_true_onehot, F):\n",
    "        \"\"\"Compute gradients and hessians for multiclass\"\"\"\n",
    "        probabilities = self._softmax(F)\n",
    "        gradients = probabilities - y_true_onehot\n",
    "        hessians = probabilities * (1 - probabilities)\n",
    "        return gradients, hessians\n",
    "    \n",
    "    def _sample_features(self, n_features):\n",
    "        \"\"\"Sample features for each tree\"\"\"\n",
    "        if self.colsample_bytree >= 1.0:\n",
    "            return np.arange(n_features)\n",
    "        \n",
    "        n_selected = max(1, int(n_features * self.colsample_bytree))\n",
    "        return np.random.choice(n_features, n_selected, replace=False)\n",
    "    \n",
    "    def _sample_data(self, n_samples):\n",
    "        \"\"\"Sample data indices for each tree\"\"\"\n",
    "        if self.subsample >= 1.0:\n",
    "            return np.arange(n_samples)\n",
    "        \n",
    "        n_selected = max(1, int(n_samples * self.subsample))\n",
    "        return np.random.choice(n_samples, n_selected, replace=False)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train LightGBM model\"\"\"\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        print(\" LIGHTGBM-LIKE TRAINING STARTED\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Setup multiclass\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.n_classes = len(self.classes_)\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        print(f\"Dataset: {n_samples} samples, {n_features} features, {self.n_classes} classes\")\n",
    "        \n",
    "        # Build histograms\n",
    "        print(\"Building feature histograms...\")\n",
    "        self.histogram_builder.build_feature_histograms(X)\n",
    "        X_binned = self.histogram_builder.map_to_bins(X)\n",
    "        print(f\"Mapped to {self.max_bins} bins per feature\")\n",
    "        \n",
    "        # Initialize predictions\n",
    "        class_counts = np.bincount(y, minlength=self.n_classes)\n",
    "        class_probs = class_counts / n_samples\n",
    "        self.initial_predictions = np.log(class_probs + 1e-15)\n",
    "        self.initial_predictions -= self.initial_predictions[0]  # First class as reference\n",
    "        \n",
    "        # Initialize F matrix\n",
    "        F = np.tile(self.initial_predictions, (n_samples, 1))\n",
    "        y_onehot = self._one_hot_encode(y)\n",
    "        \n",
    "        print(f\"🌳 Training {self.n_estimators} iterations with leaf-wise growth...\")\n",
    "        \n",
    "        # Training loop\n",
    "        for iteration in range(self.n_estimators):\n",
    "            iteration_trees = []\n",
    "            iteration_features = []\n",
    "            \n",
    "            # Compute gradients and hessians\n",
    "            gradients, hessians = self._compute_gradients_hessians(y_onehot, F)\n",
    "            \n",
    "            # Train one tree per class\n",
    "            for class_idx in range(self.n_classes):\n",
    "                # Sample data\n",
    "                sample_indices = self._sample_data(n_samples)\n",
    "                \n",
    "                # Sample features  \n",
    "                feature_indices = self._sample_features(n_features)\n",
    "                \n",
    "                # Get gradients/hessians for this class\n",
    "                class_gradients = gradients[sample_indices, class_idx]\n",
    "                class_hessians = hessians[sample_indices, class_idx]\n",
    "                \n",
    "                # Subset data\n",
    "                X_binned_subset = X_binned[np.ix_(sample_indices, feature_indices)]\n",
    "                \n",
    "                # Create tree with histogram builder for subset\n",
    "                tree_histogram_builder = HistogramBuilder(max_bins=self.max_bins)\n",
    "                \n",
    "                # Map feature indices for subset\n",
    "                for new_idx, orig_idx in enumerate(feature_indices):\n",
    "                    tree_histogram_builder.bin_mappers[new_idx] = \\\n",
    "                        self.histogram_builder.bin_mappers[orig_idx]\n",
    "                \n",
    "                # Train tree\n",
    "                tree = LightGBMTree(\n",
    "                    max_depth=self.max_depth,\n",
    "                    min_child_samples=self.min_child_samples,\n",
    "                    reg_lambda=self.reg_lambda,\n",
    "                    histogram_builder=tree_histogram_builder,\n",
    "                    max_leaves=self.num_leaves\n",
    "                )\n",
    "                \n",
    "                tree.fit(X_binned_subset, class_gradients, class_hessians)\n",
    "                \n",
    "                # Update predictions\n",
    "                tree_predictions = tree.predict(X_binned[:, feature_indices])\n",
    "                F[:, class_idx] += self.learning_rate * tree_predictions\n",
    "                \n",
    "                # Store tree\n",
    "                iteration_trees.append(tree)\n",
    "                iteration_features.append(feature_indices)\n",
    "            \n",
    "            self.trees.append(iteration_trees)\n",
    "            self.feature_indices.append(iteration_features)\n",
    "            \n",
    "            # Progress report\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                loss = self._calculate_loss(y_onehot, F)\n",
    "                print(f\"📈 Iteration {iteration+1:3d}/{self.n_estimators}, Loss: {loss:.6f}\")\n",
    "        \n",
    "        print(\"🎉 Training completed!\")\n",
    "        return self\n",
    "    \n",
    "    def _calculate_loss(self, y_true_onehot, F):\n",
    "        \"\"\"Calculate categorical cross-entropy loss\"\"\"\n",
    "        probabilities = self._softmax(F)\n",
    "        probabilities = np.clip(probabilities, 1e-15, 1 - 1e-15)\n",
    "        return -np.mean(np.sum(y_true_onehot * np.log(probabilities), axis=1))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities\"\"\"\n",
    "        X = np.array(X, dtype=np.float64)\n",
    "        X_binned = self.histogram_builder.map_to_bins(X)\n",
    "        n_samples = len(X)\n",
    "        \n",
    "        # Initialize with base predictions\n",
    "        F = np.tile(self.initial_predictions, (n_samples, 1))\n",
    "        \n",
    "        # Add predictions from all trees\n",
    "        for iteration_trees, iteration_features in zip(self.trees, self.feature_indices):\n",
    "            for class_idx, (tree, feature_indices) in enumerate(zip(iteration_trees, iteration_features)):\n",
    "                tree_pred = tree.predict(X_binned[:, feature_indices])\n",
    "                F[:, class_idx] += self.learning_rate * tree_pred\n",
    "        \n",
    "        return self._softmax(F)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return np.argmax(probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7c54981",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(model, X_test, y_test, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive model evaluation\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                               f1_score, classification_report, log_loss)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    logloss = log_loss(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"\\n{model_name} EVALUATION RESULTS:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy:    {accuracy:.4f}\")\n",
    "    print(f\"Precision:   {precision:.4f}\")\n",
    "    print(f\"Recall:      {recall:.4f}\")\n",
    "    print(f\"F1-Score:    {f1:.4f}\")\n",
    "    print(f\"Log Loss:    {logloss:.4f}\")\n",
    "    \n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'log_loss': logloss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5525ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Loading dataset...\n",
      "🏥 Disease classes: {0: 'Anemia', 1: 'Diabetes', 2: 'Healthy', 3: 'Heart Di', 4: 'Thalasse', 5: 'Thromboc'}\n",
      "\n",
      "📊 Dataset shape: 2269 train, 568 test\n",
      "TRAINING CUSTOM LIGHTGBM\n",
      "🚀 LIGHTGBM-LIKE TRAINING STARTED\n",
      "==================================================\n",
      "Dataset: 2269 samples, 24 features, 6 classes\n",
      "Building feature histograms...\n",
      "Mapped to 255 bins per feature\n",
      "🌳 Training 50 iterations with leaf-wise growth...\n",
      "📈 Iteration  10/50, Loss: 0.201449\n"
     ]
    }
   ],
   "source": [
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from lightgbm import LGBMClassifier\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load data\n",
    "    print(\" Loading dataset...\")\n",
    "    data = pd.read_csv('./Data/iniDataset.csv')\n",
    "    \n",
    "    # Prepare data\n",
    "    data.Disease = data.Disease.astype('category')\n",
    "    disease_mapping = dict(enumerate(data['Disease'].cat.categories))\n",
    "    print(f\"🏥 Disease classes: {disease_mapping}\")\n",
    "    \n",
    "    data.Disease = data.Disease.cat.codes.values\n",
    "    X = data.drop('Disease', axis=1).values\n",
    "    y = data['Disease'].values\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    print(f\"\\nDataset shape: {X_train_scaled.shape[0]} train, {X_test_scaled.shape[0]} test\")\n",
    "    \n",
    "    # Train Custom LightGBM\n",
    "    print(\"TRAINING CUSTOM LIGHTGBM\")\n",
    "\n",
    "    custom_lgbm = LightGBMClassifier(\n",
    "        n_estimators=50,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=6,\n",
    "        num_leaves=31,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=0.1,\n",
    "        max_bins=255,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    custom_lgbm.fit(X_train_scaled, y_train)\n",
    "    custom_results = evaluate_model(custom_lgbm, X_test_scaled, y_test, \"Custom LightGBM\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
